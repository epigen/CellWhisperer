{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a73622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%%\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load JSONL files into a DataFrame\n",
    "def load_jsonl_to_dataframe(file_path, fields):\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            filtered_entry = {field: entry.get(field) for field in fields}\n",
    "            data.append(filtered_entry)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# List of fields to include from each file\n",
    "fields_questions = [\"question_id\", \"text\", \"image\"]\n",
    "fields_responses = [\"question_id\", \"text\", \"model_id\"]\n",
    "fields_review = [\"id\", \"question_id\", \"content\", \"scores\"]\n",
    "\n",
    "# Load the files using the snakemake object\n",
    "questions_df = load_jsonl_to_dataframe(snakemake.input.questions, fields_questions)\n",
    "\n",
    "reference_responses_df = load_jsonl_to_dataframe(\n",
    "    snakemake.input.reference_responses, fields_responses\n",
    ")\n",
    "cw_responses_df = load_jsonl_to_dataframe(\n",
    "    snakemake.input.llava_responses[0], fields_responses\n",
    ")\n",
    "llava_responses_df = load_jsonl_to_dataframe(\n",
    "    snakemake.input.llava_responses[1], fields_responses\n",
    ")\n",
    "\n",
    "gpt4transcriptome_responses_df = load_jsonl_to_dataframe(\n",
    "    snakemake.input.gpt4transcriptome_baseline_responses, fields_responses\n",
    ")\n",
    "llava_eval_gpt4_review_df = load_jsonl_to_dataframe(\n",
    "    snakemake.input.llava_eval_gpt4_review, fields_review\n",
    ")\n",
    "\n",
    "# Merge the responses on 'question_id'\n",
    "responses = {\"question\": questions_df.set_index(\"question_id\")[\"text\"]}\n",
    "for response in [\n",
    "    reference_responses_df,\n",
    "    cw_responses_df,\n",
    "    llava_responses_df,\n",
    "    gpt4transcriptome_responses_df,\n",
    "]:\n",
    "    assert response.model_id.nunique() == 1\n",
    "    model_id = response.model_id.iloc[0]\n",
    "    responses[f\"response_{model_id}\"] = response.set_index(\"question_id\")[\"text\"]\n",
    "\n",
    "\n",
    "responses = pd.DataFrame(responses)\n",
    "responses[\"review\"] = llava_eval_gpt4_review_df.set_index(\"question_id\")[\"content\"]\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791bd50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "score_df = pd.DataFrame.from_records(\n",
    "    llava_eval_gpt4_review_df[\"scores\"], index=llava_eval_gpt4_review_df.question_id\n",
    ")\n",
    "score_df.columns = [c.replace(\"generation_\", \"score_\") for c in score_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.join(score_df).to_excel(snakemake.output.llava_eval_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
