{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecc457",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava import conversation as conversation_lib\n",
    "\n",
    "from snakemake.io import Namedlist as SnakemakeNamedlist\n",
    "\n",
    "from llava.train.train import (\n",
    "    LazySupervisedDataset,\n",
    "    DataArguments,\n",
    "    DataCollatorForSupervisedDataset,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer, EvalPrediction\n",
    "import transformers\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Setup logging with INFO level and file snakemake.log.log\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    handlers=[\n",
    "        logging.FileHandler(snakemake.log.log),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "model_dir = (\n",
    "    snakemake.input.llava_model\n",
    ")  # \"/msc/home/mschae83/cellwhisperer/results/llava/finetuned/Mistral-7B-Instruct-v0.2__03jujd8s/\"\n",
    "evaluation_dataset_fn = (\n",
    "    snakemake.input.evaluation_dataset\n",
    ")  # \"/msc/home/mschae83/cellwhisperer/results/llava_evaluation_conversations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c1dba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with open(evaluation_dataset_fn) as f:\n",
    "    eval_set = json.load(f)\n",
    "eval_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2612c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if isinstance(snakemake.input.top_genes, SnakemakeNamedlist):\n",
    "    top_genes = pd.concat(\n",
    "        [\n",
    "            pd.read_parquet(fn).iloc[:, : snakemake.params.top_n_genes]\n",
    "            for fn in snakemake.input.top_genes\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    top_genes = pd.read_parquet(snakemake.input.top_genes).iloc[:, : snakemake.params.top_n_genes]  # type: ignore [reportUndefinedVariable]\n",
    "top_genes.head()\n",
    "# .dropna()  # TODO necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "# TODO make sure load_pretrained_model is flexible enough.\n",
    "\n",
    "model_name = get_model_name_from_path(model_dir)\n",
    "assert (\n",
    "    \"mistral\" in model_name.lower() or \"llama\" in model_name.lower()\n",
    "), \"sure that you are not using a mistral model? LLaVA depends on having it in the name (if it is mistral)\"\n",
    "\n",
    "\n",
    "if \"__\" not in model_name:\n",
    "    logger.warning(\"'__' not in model_name. Could lead to unforseen consequences.\")\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_dir,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    load_8bit=False,\n",
    "    load_4bit=False,\n",
    "    device=\"cuda\",\n",
    "    use_flash_attn=False,\n",
    ")\n",
    "\n",
    "\n",
    "logger.info(f\"Loaded model {model_name} from {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    tokenizer.pad_token is None\n",
    "):  # needs to be set explicitly for some non-multimodal models\n",
    "    if tokenizer.unk_token_id is None:\n",
    "        tokenizer.add_special_tokens({\"unk_token\": \"<unk>\"})\n",
    "        model.model.config.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "        orig_embed = model.model.embed_tokens\n",
    "        model.model.embed_tokens = nn.Embedding(\n",
    "            orig_embed.weight.shape[0] + 1,\n",
    "            orig_embed.weight.shape[1],\n",
    "            padding_idx=tokenizer.unk_token_id,\n",
    "            dtype=model.model.embed_tokens.weight.dtype,\n",
    "            device=model.model.embed_tokens.weight.device,\n",
    "        )\n",
    "        model.model.embed_tokens.weight.data[:-1] = orig_embed.weight.data\n",
    "        nn.init.zeros_(model.model.embed_tokens.weight.data[-1:, :])\n",
    "\n",
    "    tokenizer.pad_token_id = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf194738",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\n",
    "    \"mistral_instruct\" if \"mistral\" in model_name.lower() else \"llama3_instruct\"\n",
    "]\n",
    "\n",
    "eval_dataset = LazySupervisedDataset(\n",
    "    evaluation_dataset_fn,\n",
    "    tokenizer,\n",
    "    DataArguments(\n",
    "        image_data=snakemake.input.image_data,\n",
    "        mm_vision_select_layer=snakemake.params.model_layer_selector,\n",
    "    ),\n",
    ")\n",
    "assert (\n",
    "    len(eval_dataset) > 0\n",
    "), \"Something is wrong with the input data: LazySupervisedDataset sees 0 data points.\"\n",
    "\n",
    "logger.info(\n",
    "    f\"Loaded evaluation dataset with {len(eval_dataset)} data points. First one: {eval_dataset[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52575ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.list_data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datapoint in eval_dataset.list_data_dict:\n",
    "    if not snakemake.params.is_multimodal:\n",
    "        datapoint[\"conversations\"][0][\"value\"] = (\n",
    "            datapoint[\"conversations\"][0][\"value\"]\n",
    "            .replace(\"\\n<image>\", \"\")\n",
    "            .replace(\"<image>\\n\", \"\")\n",
    "        )\n",
    "    if snakemake.params.pre_prompt_topgenes:  # type: ignore [reportUndefinedVariable]\n",
    "        pre_prompt = copy.deepcopy(snakemake.params.pre_prompt_topgenes)\n",
    "        pre_prompt[0][\"value\"] = pre_prompt[0][\"value\"].format(\n",
    "            \", \".join(top_genes.loc[datapoint[\"id\"]])\n",
    "        )\n",
    "        datapoint[\"conversations\"] = pre_prompt + datapoint[\"conversations\"]  # type: ignore [reportUndefinedVariable]\n",
    "\n",
    "eval_dataset.list_data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.list_data_dict[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66329d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef60ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator([eval_dataset[5], eval_dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35653e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss with CrossEntropyLoss, which expects raw logits, not probabilities\n",
    "loss_fct = CrossEntropyLoss(reduction=\"none\")  # ignores -100 token implicitly\n",
    "\n",
    "\n",
    "def _create_last_response_block_mask(labels):\n",
    "    mask = []\n",
    "    for i, row in enumerate(labels):\n",
    "        row_mask = torch.zeros_like(row)\n",
    "        indices = torch.nonzero(row == -100, as_tuple=False)\n",
    "        max100 = indices.max()\n",
    "        if max100 == len(row) - 1:\n",
    "            in_block = True\n",
    "            for j in range(len(row) - 1, 0, -1):\n",
    "                if in_block:\n",
    "                    if row[j] == -100:\n",
    "                        continue\n",
    "                    else:\n",
    "                        in_block = False\n",
    "                        row_mask[j] = 1\n",
    "                else:\n",
    "                    if row[j] == -100:\n",
    "                        break\n",
    "                    else:\n",
    "                        row_mask[j] = 1\n",
    "        else:\n",
    "            row_mask[max100 + 1 :] = 1\n",
    "\n",
    "        mask.append(row_mask)\n",
    "\n",
    "    mask = torch.stack(mask).to(bool)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(\n",
    "    logits: torch.Tensor, labels: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    if snakemake.params.is_multimodal:\n",
    "        # num_projector_tokens is `1` if is_multimodal is False\n",
    "        shift_logits = logits[\n",
    "            ..., snakemake.params.num_projector_tokens - 1 : -1, :\n",
    "        ].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "    else:\n",
    "        assert -200 not in labels\n",
    "\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    attention_mask = _create_last_response_block_mask(shift_labels)\n",
    "\n",
    "    # Only compute loss where attention_mask is true\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    loss = loss.view(shift_labels.size())\n",
    "\n",
    "    # Apply the attention mask to exclude ignored indices from the loss calculation\n",
    "    masked_loss = torch.where(attention_mask, loss, torch.tensor(0.0).to(loss.device))\n",
    "\n",
    "    # Sum the loss per example and divide by the number of non-ignored tokens to get the loss per example\n",
    "    example_losses = torch.sum(masked_loss, dim=1)\n",
    "    example_lengths = torch.sum(attention_mask, dim=1)\n",
    "    example_perplexities = torch.exp(example_losses / example_lengths)\n",
    "\n",
    "    return example_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already defined `model`, `tokenizer`, `eval_dataset`, and `data_collator`\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_pred: {\n",
    "        \"perplexity\": eval_pred.predictions.mean().item(),\n",
    "        \"all_perplexities\": eval_pred.predictions,\n",
    "    },\n",
    "    args=transformers.TrainingArguments(\n",
    "        report_to=\"none\",\n",
    "        output_dir=\"/tmp\",\n",
    "        eval_accumulation_steps=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "    ),\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    # batch_eval_metrics=True\n",
    ")\n",
    "logger.info(\"Starting evaluation (correct)\")\n",
    "# Evaluate the model\n",
    "correct_results = trainer.evaluate()\n",
    "\n",
    "correct_results[\"response\"] = [\n",
    "    conv[\"conversations\"][-1][\"value\"] for conv in eval_dataset.list_data_dict\n",
    "]\n",
    "correct_results[\"type\"] = \"correct\"\n",
    "correct_results[\"question_id\"] = list(\n",
    "    range(len(correct_results[\"eval_all_perplexities\"]))\n",
    ")\n",
    "correct_results[\"replicate\"] = \"-1\"\n",
    "logger.info(\"Finished evaluation (correct)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66296bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[conv[\"conversations\"][-1][\"value\"] for conv in eval_dataset.list_data_dict][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0299f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.list_data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ed012",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: would be much more elegant to 'modify the \"image\" field' rather than the orig_id_to_int_stable. But it's implemented and it works\n",
    "\n",
    "incorrect_results = []\n",
    "\n",
    "orig_id_to_int_stable = eval_dataset.orig_id_to_int.copy()\n",
    "\n",
    "true_responses = [\n",
    "    conv[\"conversations\"][-1][\"value\"] for conv in eval_dataset.list_data_dict\n",
    "]\n",
    "possible_responses = list(set(true_responses))\n",
    "\n",
    "if snakemake.params.background_shuffle == \"llm-response\":\n",
    "    num_negatives = len(possible_responses) - 1\n",
    "else:\n",
    "    num_negatives = snakemake.params.num_negatives\n",
    "\n",
    "\n",
    "for i in range(num_negatives):\n",
    "    logger.info(f\"Begin evaluation (incorrect) replicate {i}\")\n",
    "    # Calculate background perplexity (i.e. with the wrongly matched transcriptome)\n",
    "    random.seed(\n",
    "        i\n",
    "    )  # something seems to reset the seed on every iteration, so we need to force it differently for random.choice\n",
    "\n",
    "    if snakemake.params.background_shuffle == \"transcriptome\":\n",
    "        # eval_dataset.orig_id_to_int = {k: (v+2)%len(eval_dataset.orig_id_to_int) for k, v in eval_dataset.orig_id_to_int.items()}  # <- deprecated\n",
    "        for conv in eval_dataset.list_data_dict:\n",
    "            # swap out the embedding (no effect for non-multimodal)\n",
    "            true_id = conv[\"id\"]\n",
    "            true_response = conv[\"conversations\"][-1][\"value\"]\n",
    "\n",
    "            mismatch_response_id = random.choice(\n",
    "                [\n",
    "                    e[\"id\"]\n",
    "                    for e in eval_dataset.list_data_dict\n",
    "                    if e[\"conversations\"][-1][\"value\"] != true_response\n",
    "                ]\n",
    "            )\n",
    "            eval_dataset.orig_id_to_int[true_id] = orig_id_to_int_stable[\n",
    "                mismatch_response_id\n",
    "            ]\n",
    "\n",
    "            # swap out the top n genes in the same manner\n",
    "            if snakemake.params.pre_prompt_topgenes:\n",
    "                pre_prompt = copy.deepcopy(snakemake.params.pre_prompt_topgenes)\n",
    "                pre_prompt[0][\"value\"] = pre_prompt[0][\"value\"].format(\n",
    "                    \", \".join(top_genes.loc[mismatch_response_id])\n",
    "                )\n",
    "                # replace pre-prompt with wrong one\n",
    "                conv[\"conversations\"] = (\n",
    "                    pre_prompt + conv[\"conversations\"][len(pre_prompt) :]\n",
    "                )\n",
    "\n",
    "            logger.debug(\n",
    "                f\"Produced mismatch conversation with mismatch id {mismatch_response_id}: {conv}\"\n",
    "            )\n",
    "    elif snakemake.params.background_shuffle == \"genesshuffled\":\n",
    "        # Only shuffle the gene order\n",
    "        assert snakemake.params.pre_prompt_topgenes, \"Need top genes for this shuffle\"\n",
    "        for conv in eval_dataset.list_data_dict:\n",
    "            true_id = conv[\"id\"]\n",
    "\n",
    "            # shuffle the top n genes:\n",
    "            pre_prompt = copy.deepcopy(snakemake.params.pre_prompt_topgenes)\n",
    "            pre_prompt[0][\"value\"] = pre_prompt[0][\"value\"].format(\n",
    "                \", \".join(top_genes.loc[true_id].sample(frac=1))\n",
    "            )\n",
    "            # replace pre-prompt with shuffled one\n",
    "            conv[\"conversations\"] = (\n",
    "                pre_prompt + conv[\"conversations\"][len(pre_prompt) :]\n",
    "            )\n",
    "\n",
    "    elif snakemake.params.background_shuffle == \"responsepermuted\":\n",
    "        for true_response, conv in zip(true_responses, eval_dataset.list_data_dict):\n",
    "            # Generate an incorrect (shuffled) response (retain the source embedding)\n",
    "            if snakemake.wildcards.dataset.endswith(\"_top50genes\"):\n",
    "                # Shuffle the order of the response genes\n",
    "                comma_split = true_response.rsplit(\", \", maxsplit=50)\n",
    "\n",
    "                prefix, first_gene = comma_split[0].rsplit(\" \", maxsplit=1)\n",
    "\n",
    "                genes = [first_gene] + comma_split[1:]\n",
    "                shuffled = prefix + \" \" + \", \".join(random.sample(genes, len(genes)))\n",
    "            else:\n",
    "                response_words = true_response.rsplit(\" \")\n",
    "                shuffled = \" \".join(random.sample(response_words, len(response_words)))\n",
    "            conv[\"conversations\"][-1][\"value\"] = shuffled\n",
    "            logger.debug(f\"Shuffled response '{true_response}' to '{shuffled}'\")\n",
    "    elif snakemake.params.background_shuffle == \"llm-response\":\n",
    "        for true_response, conv in zip(true_responses, eval_dataset.list_data_dict):\n",
    "            conv[\"conversations\"][-1][\"value\"] = [\n",
    "                v for v in possible_responses if v != true_response\n",
    "            ][i]\n",
    "    else:\n",
    "        raise ValueError(snakemake.params.background_shuffle)\n",
    "\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: {\n",
    "            \"perplexity\": eval_pred.predictions.mean().item(),\n",
    "            \"all_perplexities\": eval_pred.predictions,\n",
    "        },\n",
    "        args=transformers.TrainingArguments(\n",
    "            report_to=\"none\", output_dir=\"/tmp\", eval_accumulation_steps=4\n",
    "        ),\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "    incorrect_result = trainer.evaluate()\n",
    "    incorrect_result[\"type\"] = \"incorrect\"\n",
    "    incorrect_result[\"replicate\"] = i\n",
    "    incorrect_result[\"response\"] = [\n",
    "        conv[\"conversations\"][-1][\"value\"] for conv in eval_dataset.list_data_dict\n",
    "    ]\n",
    "    incorrect_result[\"question_id\"] = list(\n",
    "        range(len(incorrect_result[\"eval_all_perplexities\"]))\n",
    "    )\n",
    "    incorrect_results.append(incorrect_result)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e65a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [pd.DataFrame(correct_results)]\n",
    "    + [pd.DataFrame(incorrect_result) for incorrect_result in incorrect_results]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(snakemake.output.all_perplexities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
