"""
CellWhisperer LLM validations based on

- perplexity

Next better evaluations could include

- how many of the top 100 genes can it recover?
- how well does it reproduce pathways?
"""

import pandas as pd


scattergather:  # same as in llava pipeline
    split=128

# This list is generated by the llava pipeline and, for convenience, tracked as part of the source code
ARCHS4_GSVA_SAMPLES = pd.read_csv(PROJECT_DIR / "src/llava/gsva_samples.csv", header=None).iloc[:, 0]

NUM_COMPLEX_SAMPLES = 5000
NUM_DETAILED_SAMPLES = 10000
CONVERSATION_START = NUM_COMPLEX_SAMPLES + NUM_DETAILED_SAMPLES
COMPLEX_SAMPLES = ARCHS4_GSVA_SAMPLES.to_list()[:NUM_COMPLEX_SAMPLES]
DETAILED_SAMPLES = ARCHS4_GSVA_SAMPLES.to_list()[NUM_COMPLEX_SAMPLES:CONVERSATION_START]


def top_genes_fn(wildcards):
    if wildcards.dataset in ["main", "main_top50genes"]:
        return [ancient(rules.compute_top_genes.output.top_genes.format(dataset=dataset)) for dataset in ["cellxgene_census", "archs4_geo"]]
    else:
        return [rules.compute_top_genes.output.top_genes.format(dataset=wildcards.dataset.replace("_top50genes", ""))]


rule llava_celltype_evaluation_dataset:
    """
    '_celltype' is lower-case to avoid confusing the model
    """
    input:
        dataset=PROJECT_DIR / config["paths"]["read_count_table"],
    output:
        _default=PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"].format(dataset="{dataset}", llava_dataset="_default"),
        _celltype=PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"].format(dataset="{dataset}", llava_dataset="_celltype"),
        _top50genescelltype=PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"].format(dataset="{dataset}", llava_dataset="_top50genescelltype"),
    params:
        num_cells_per_celltype=None,  # `None` corresponds to *all* cells. The input dataset is already downsampled to 100 cells per cell type.
        question=config["llava_eval"]["question_celltype"],
        response_prefix=config["llava_eval"]["response_prefix_celltype"],
    resources:
        mem_mb=90000,
    conda:
        "cellwhisperer"
    notebook:
        "../notebooks/llava_celltype_evaluation_dataset.py.ipynb"



rule llava_evaluation_topgenes_dataset:
    input:
        dataset=lambda wildcards: str(PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"]).format(dataset=wildcards.dataset.replace("_top50genes", ""), llava_dataset=wildcards.llava_dataset),
        top_genes=top_genes_fn
    output:
        evaluation_dataset=PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"].replace("{dataset}", "{dataset,.+_top50genes}"),
    params:
        question=config["llava_eval"]["question_topgenes"],
        response_prefix=config["llava_eval"]["response_prefix_topgenes"],
        top_n_genes=50,
    resources:
        mem_mb=90000,
    conda:
        "cellwhisperer"
    notebook:
        "../notebooks/llava_evaluation_topgenes_dataset.py.ipynb"

rule llava_evaluation_perplexity:
    """
    NOTE: slight limitation: When applying preprompts, we measure the perplexity of both the preprompt-response and the real response. This introduces a little bit of noise, but probably no bias.
    """
    input:
        llava_model=lambda wildcards:
            (PROJECT_DIR / config["paths"]["llava"]["finetuned_model_dir"].format(base_model=wildcards.base_model, model=wildcards.model, llava_dataset=wildcards.llava_dataset))
            if wildcards.model != "NONE" else
            PROJECT_DIR / "resources" / wildcards.base_model,
        evaluation_dataset=PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"],
        # image_data=rules.process_full_dataset.output.model_outputs.format(dataset="{dataset}", model=config["model_name_path_map"]["cellwhisperer"]),
        image_data=lambda wildcards: rules.combine_processed_data.output.combined.format(model=wildcards.model.replace("NONE", "cellwhisperer_clip_v1")),  # For NONE, we don't need any image_data, but we do need to provide a file
        top_genes=top_genes_fn  # only required for `prompt_variation="with50topgenes"`. not sure if `ancient` is correct
    conda:
        "llava"
    output:
        all_perplexities=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "all_perplexities.csv",
    params:
        num_projector_tokens=int(config["llava_projector_type"].split("_")[1].strip("t")),
        background_shuffle=lambda wildcards: ("responsepermuted" if "responsepermuted" in wildcards.prompt_variation
                                              else (
                                                      "llm-response" if "response" in wildcards.prompt_variation
                                                      else (
                                                              "genesshuffled" if "shuffled" in wildcards.prompt_variation
                                                              else "transcriptome"
                                                      )
                                              )
                                              ),
        num_negatives=30,  # unused for `llm-response` to get all label-probabilities.
        model_layer_selector=-1,
        pre_prompt_topgenes=lambda wildcards: config["llava_eval"]["pre_prompt_topgenes"] if "with50topgenes" in wildcards.prompt_variation else None,
        top_n_genes=50,
        is_multimodal=lambda wildcards: not (wildcards.model == "NONE" or "noembedding" in wildcards.prompt_variation)
    resources:
        mem_mb=200000,
        slurm=lambda wildcards: slurm_gres(
            "large",
            num_gpus={LLAVA_BASE_MODEL: 1, "Llama-3.3-70B-Instruct": 4}[wildcards.base_model]
        )
    log:
        notebook="logs/llava_evaluation_perplexity/{dataset}_{base_model}_{model}_{llava_dataset}{prompt_variation}.ipynb",
        log="logs/llava_evaluation_perplexity/{dataset}_{base_model}_{model}_{llava_dataset}{prompt_variation}.log"
    threads: 16
    notebook:
        "../notebooks/llava_evaluation_perplexity.py.ipynb"



rule llava_evaluation_perplexity_plots:
    """
    """
    input:
        all_perplexities=rules.llava_evaluation_perplexity.output.all_perplexities,
        mpl_style=ancient(PROJECT_DIR / config["plot_style"])
    output:
        log_perplexity_ratio=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "log_mean_perplexity.ratio",  # smaller is better log(ppl_real/ppl_neg_control)
        comparison_plot=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "perplexity_quantile.svg",  # barplot
        detailed_plot=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "detailed.svg",  # barplot
        full_supp_table=PROJECT_DIR / config["paths"]["llava"]["root"] / "{dataset}" / "{base_model}__{model}" / "quantile_stats_{prompt_variation}.xlsx",
    params:
        plot_celltypes=config["top20_lung_liver_blood_celltypes"],
        response_prefix=lambda wildcards: config["llava_eval"]["response_prefix_{}".format(
            "topgenes" if "_top50genes" in wildcards.dataset else "celltype")]
    conda:
        "llava"  # newer version of pandas in this env
    log:
        notebook="logs/llava_evaluation_perplexity_plots/{dataset}_{base_model}_{model}_{llava_dataset}{prompt_variation}.ipynb"
    notebook:
        "../notebooks/llava_evaluation_perplexity_plots.py.ipynb"

rule llava_evaluation_prediction_scores:
    """
    Only works for `response` predictions, as we only have complete predictions for all possible labels for these. (hence the regex in the output rule)
    """
    input:
        all_perplexities=rules.llava_evaluation_perplexity.output.all_perplexities,
    output:
        predictions_raw=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "predictions_raw.csv",  # probabilities.
        predictions=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "predictions.csv",  # columns: `,predicted_labels,valid_prediction,label,is_correct` (len is len(dataset), e.g. 15159)
        performance=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "performance.csv",
    wildcard_constraints:
        propmt_variation=".*response(?!permuted).*",
    conda:
        "llava"  # newer version of pandas in this env
    resources:
        mem_mb=30000,
    log:
        notebook="logs/llava_evaluation_prediction_scores/{dataset}_{base_model}_{model}_{llava_dataset}{prompt_variation}.ipynb"
    notebook:
        "../notebooks/llava_evaluation_prediction_scores.py.ipynb"


def input_configurations(wildcards):
    if wildcards.plot_type == "llava_cw_vs_geneformer":
        return [
            {"base_model": LLAVA_BASE_MODEL, "model": "geneformer", "prompt_variation": "without50topgenes"},
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "without50topgenes"},
        ]
    if wildcards.plot_type == "gene_predictability":
        if not wildcards.dataset.endswith("top50genes"):
            print("gene_predictability is only well-defined for top_50gene dataset")

        return [
            {"base_model": LLAVA_BASE_MODEL, "model": "NONE", "prompt_variation": "without50topgenes"},  #  negative control
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "without50topgenes"},  # normal performance (consider hiding this)
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "without50topgenesresponsepermuted"},  # different background distribution
        ]
    if wildcards.plot_type == "text_only_vs_cw":
        if wildcards.dataset.endswith("top50genes"):
            print("text_only_vs_cw doesn't make sense with top50genes datasets")
        return [
            {"base_model": LLAVA_BASE_MODEL, "model": "NONE", "prompt_variation": "with50topgenesresponse"},
            {"base_model": "Llama-3.3-70B-Instruct", "model": "NONE", "prompt_variation": "with50topgenesresponse"},
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "without50topgenesresponse"},
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "with50topgenesresponse"},  # just added this for curiosity
        ]
    if wildcards.plot_type == "cw_preprompt_useless":
        if wildcards.dataset.endswith("top50genes"):
            print("cw_preprompt_useless doesn't make sense with top50genes datasets")
        return [
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "noembedding"},  # Baseline: CW with no input at all
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "with50topgenesnoembedding"},  # Baseline: CW with only top 50 genes
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "without50topgenes"},
            {"base_model": LLAVA_BASE_MODEL, "model": "cellwhisperer_clip_v1", "prompt_variation": "with50topgenes"},  # show that adding 50 top genes doesn't help (much)
        ]


rule llava_comparative_perplexity_plots:
    """
    General comparative plots for llava perplexity analysis

    Relative plots: compare pairwise and show which one is better how often (matrix)

    See notebook for by-celltype plot (only TabSap)
    """
    input:
        perplexities=lambda wildcards: [
            rules.llava_evaluation_perplexity.output.all_perplexities.format(**x, dataset=wildcards.dataset, llava_dataset="_default")
            for x in input_configurations(wildcards)
        ],
        evaluation_dataset=PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"].format(llava_dataset="_default", dataset="{dataset}"),
        mpl_style=ancient(PROJECT_DIR / config["plot_style"]),
    output:
        individual_performances=PROJECT_DIR / "results/plots/llava_default/{dataset}/ppl_model_comparison/{plot_type}/{plot_metric}_individual.svg",
        relative_performances=PROJECT_DIR / "results/plots/llava_default/{dataset}/ppl_model_comparison/{plot_type}/{plot_metric}_relative.svg",
    params:
        comparison_sequence=input_configurations,
        plot_celltypes=config["top20_lung_liver_blood_celltypes"],
        response_prefix=lambda wildcards: config["llava_eval"]["response_prefix_{}".format(
            "topgenes" if "_top50genes" in wildcards.dataset else "celltype")],
        plot_type="violin"  # lambda wildcards: "violin" if wildcards.dataset.startswith("tabula_sapiens") else "violinstrip",
    resources:
        mem_mb=50000,
    conda:
        "llava"
    notebook:
        "../notebooks/llava_comparative_perplexity_plots.py.ipynb"

rule fig4_llava_ppl_all:
    input:
        # Evaluation compared to mismatched conversations
        expand(PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "{plot_name}.svg",
               plot_name=["detailed", "perplexity_quantile"],
               dataset=["main", "tabula_sapiens_100_cells_per_type"],  # these are llava evaluation datasets
               base_model=[LLAVA_BASE_MODEL],
               model=[config["model_name_path_map"]["cellwhisperer_geneformer"]],
               prompt_variation=["without50topgenes", "without50topgenesresponse"],  # default
               llava_dataset=["_default"],
               ),

        # Comparison benchmark
        expand(
            rules.llava_comparative_perplexity_plots.output.individual_performances,
            plot_metric=["log2_correct_ppl"],
            plot_type=["text_only_vs_cw"],
            dataset=["main", "tabula_sapiens_100_cells_per_type"],  # these are llava evaluation datasets
        ),
