{
    "default":  {"role": "Assistant", "prompt": "We would like to request your feedback on the performance of multiple AI assistants in response to a question indicated in the [Question] block. The question refers to the state of a cell exhibited represented through its transcriptome. A ground truth description of this transcriptome state is provided in text form in the [Reference] block. It is comprised of a succinct natural language summary of the biological state of the sample and a prioritized list of the most prominently expressed genes (in descending order).\nFirst, comprehensively assess the responses of each of the assistants with a focus on their relevance, accuracy, conciseness and level of detail, always with respect to the ground truth reference information. Make sure to detect and recognize 'honest mistakes' (e.g. the correct detection of a treatment, but with a slightly different (but related) compound; or the detection of a wrong but similar cell type, etc.) and judge them more generously than a completely wrong answer. Also, don't penalize information that goes beyond the provided reference, as long as it does not contradict it.\nBased on this assessment, provide one overall score per assistant on a scale of 1 to 10, with 1 indicating the worst of the provided responses and 10 indicating the best one. Here is the JSON template for your response: {\"explanation\": <explanation:str>, \"assistant_1\": <1-10:int>, \"assistant_2\": <1-10:int>, \"assistant_3\": <1-10:int>, ...}"}
}
